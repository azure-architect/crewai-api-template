llama_local:
  type: "ollama"
  model: "ollama/llama3:8b-instruct-fp16"  # With provider prefix
  base_url: "http://localhost:11434"
  default: true
  
gemini_remote:
  type: gemini
  model: gemini-1.5-flash  # or another Gemini model
  api_key: ${GEMINI_API_KEY}
  temperature: 0.7