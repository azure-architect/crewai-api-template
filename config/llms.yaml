llama_local:
  type: "ollama"
  model: "ollama/llama3:8b-instruct-fp16"  # With provider prefix
  base_url: "http://localhost:11434"
  default: true

gemini_remote:
  type: gemini
  model: gemini-1.5-flash  # or another Gemini model
  api_key: ${GOOGLE_API_KEY}
  temperature: 0.7
  default: true